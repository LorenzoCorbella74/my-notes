# Mistral AI
[Mistral](https://mistral.ai/en) Ã¨ una societÃ  che produce "foundational models" che nel 2023 Ã¨ appasa sul mercato competendo con OpenAI. Ha un [chat assistant](https://chat.mistral.ai/chat) chiamato Lucia che permette di interagire con il modello. Si puÃ² interagire con il modello anche tramite - API Endopoint (bisogna prendere una API KEY).
- Cloud services (che forniscono un'infrastruttura.scalabile e sicura per eseguire modelli di IA)
- Infrastrutture on premise

## Indice corso
- Eseguire modelli localmente
- API e JS SDK (esiste anche il python SDK)
- Modelli
- Embeddings
- Vector databases (per dare alle app il cosÃ¬ detto DOMAIN KNOLEDGE, come dati proprietari e real time information per cui il modello non Ã¨ in precedenza stato allenato o informazioni extra)
- **Retrieval augmented generation** (RAG)
- **AI Agents with function calling** che permettono alle app di prendere azioni in base all'user prompt


## Mistral Models
[I modelli di Mistral](https://mistral.ai/en/models#models) sono:
- mistral7B (open source)
- mistral8x7 (open source mix di esperti)
- small, medium, large  (enterprice grade), multi-lingual models con function calling, JSON mode 32K context window + un modello per embeddings.

E' possibile accedere ai modelli tramite API endpoints (La plateforme) o Public Cloud o On-premise. I modelli vanno in termini di percentuale MMLU (Massive multitask language understanding) dal 62,5% di Mistral7b al 81,2% di Mistra-Large.

**NB:** Hugging face Ã¨ una piattaforma ricca di modelli open source e dataset Ã¨ [Hugging Face](https://huggingface.co/) su cui Ã¨ possibile recuperare vari modelli di [mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)


# Utlizzo
Per poter utilizzare l'API bisogna recuperare una [API keys](https://console.mistral.ai/api-keys/)

## Chat completion API
Per utilizzare il modello di chat completion in Javascriptsi puÃ² eseguire il codice seguente:

```typescript
import MistralClient from '@mistralai/mistralai';

const apiKey = process.env.MISTRAL_API_KEY || 'your_api_key';
const client = new MistralClient(apiKey);

const chatResponse = await client.chat({
  model: 'mistral-tiny',
  messages: [{ role: 'user', content: 'What is the best French cheese?' }],
  temperature: 1 // livello di creativitÃ  del modello (0 Ã¨ il piÃ¹ deterministico, 1 Ã¨ il piÃ¹ creativo, 0.7 Ã¨ il default)
});

console.log(chatResponse.choices[0].message.content);
```

Da notare che per guidare il comportamento del modello si possono aggiungere dei "system prompts":
```typescript

const chatResponse = await client.chat({
    model: 'mistral-tiny',
  messages: [
      { 
        role: 'system', 
        content: 'sei un appassionato di formaggio e quando ti verrÃ  richiesto di formaggi, rispondi concisamente e con humour' 
      },
      { role: 'user', content: 'What is the best French cheese?' }
    ],
  temperature: 1,
});

```
Le opzioni prevedono di aggiungere il:
-  supporto per lo streaming. 

```typescript

const chatResponse = await client.chatStream({
    model: 'mistral-tiny',
  messages: [
      { 
        role: 'system', 
        content: 'sei un appassionato di formaggio e quando ti verrÃ  richiesto di formaggi, rispondi concisamente e con humour' },
      { role: 'user', content: 'What is the best French cheese?' }
    ],
  temperature: 1,
});

// chatResponse Ã¨ un oggetto Iterable
for await (const chunk of chatResponse) {   
    console.log(chunk.choices[0].delta.content);
} 
```
E' possibile avere la risposta in formato JSON:
```typescript
const chatResponse = await client.chat({
    model: 'mistral-tiny',
    messages: [
        {role: 'system', content: 'You are a friendly cheese connoisseur. When asked about cheese, reply concisely and humorously. Reply with JSON.'},
        {role: 'user', content: 'What is the best French cheese?'}
    ],
    temperature: 0.5,
    response_format: {
        type: "json_object"
    }
});

// '{ "answer": "Why, that's like asking which child is your favorite! But if I must pick, I'd go with Brie de Meaux. It's a classic, like a good French beret or an accent.", "cheese": { "name": "Brie de Meaux", "country": "France", "type": "Soft" } }'
console.log(chatResponse.choices[0].message.content);
```




## RAG
RAG (Retrieval-Augmented Generation) Ã¨ una tecnica che combina il recupero di informazioni (retrieval) con la generazione di testo (generation) per migliorare la qualitÃ  e l'accuratezza delle risposte di un modello di linguaggio.

ğŸ”¹ Come funziona RAG?
Recupero delle informazioni (Retrieval) â†’ Il modello cerca dati aggiornati o rilevanti da fonti esterne (documenti, database, web, API).
![retrieval](./mistral_img1.png)
Una volta rerecuperati i dati si conservano in un vector DB (ossia in un formato, i cosÃ¬ detti **embeddings** che puÃ² essere facilmente utilizzato dal modello) e si possono fare delle query (anch'esse messe in un formato embedded) per avere un risultato  come prodotto di un semantic search. I risultai della ricerca e l'user prompt vengono poi passati al modello di generazione come un singolo prompt che il modello LLM utilizza come input.
![generation](./mistral_img2.png)

Generazione della risposta (Generation) â†’ L'LLM usa queste informazioni per formulare una risposta contestuale e accurata.

Esempi pratici:
1. Rispondere con dati aggiornati
Senza RAG â†’ Un LLM potrebbe basarsi su dati pre-addestrati e dare informazioni obsolete.
Con RAG â†’ Il modello cerca dati in tempo reale, ad esempio da Google o un database interno.
ğŸ‘‰ "Chi Ã¨ il presidente degli USA?"
  - Normale LLM â†’ "Secondo i miei dati, il presidente Ã¨ Joe Biden." (potenzialmente obsoleto)
  - LLM con RAG â†’ Recupera la risposta da una fonte aggiornata: "Il presidente attuale Ã¨ [Nome]."

2. Chatbot aziendali con documentazione interna
Un'azienda ha un assistente virtuale per supporto tecnico.
ğŸ‘‰ "Come resetto la password del CRM?"
- Normale LLM â†’ Potrebbe dare istruzioni generiche.
- LLM con RAG â†’ Recupera la guida piÃ¹ recente dall'intranet aziendale e fornisce istruzioni precise.

In definitiva i vantaggi sono:
- âœ… Risposte piÃ¹ aggiornate e affidabili
- âœ… Meno allucinazioni (errori nei dati)
- âœ… Maggiore adattabilitÃ  per domande specifiche

RAG Ã¨ particolarmente utile per chatbot, motori di ricerca intelligenti e assistenti aziendali!

Gli **embeddings** sono il modo per rappresentare testi, immagini o altri dati in una forma numerica che un computer puÃ² elaborare facilmente. In parole semplici, sono liste di numeri (vettori) che catturano il significato  di parole, frasi o documenti e loro relazioni in uno spazio matematico (che non ha 2 dimentsioni ma spesso centinaia di dimensioni...).
PerchÃ¨ servono? I computer non capiscono il linguaggio naturale direttamente, quindi dobbiamo trasformare le parole in numeri in modo che:
- âœ… Parole simili abbiano rappresentazioni simili.
- âœ… Possiamo confrontare testi e trovare relazioni tra loro e categorizzare .
- âœ… Possiamo cercare documenti in base al significato e non solo alle parole esatte.


L'idea Ã¨ quella di frazionare il testo in parti di testo + piccole (chunk) e di produrre gli embedding di tali parti di testo. Di solito + Ã¨ corto il chunk e miglioreÃ¨ il significato anche se potrebbe perdere un contesto esteso su piÃ¹ frasi.  Questi embedding sono poi usati per fare una query nel DB per trovare i chunk piÃ¹ simili al prompt dato e costruire un prompt con la query originale + i chunk e passare la query finale al modello.

```js
async function splitDocument() {
    const response = await fetch('handbook.txt');
    const text = await response.text();
    console.log(text);
}

splitDocument();
```
Si usa un framework come [Langchain](https://js.langchain.com/v0.1/docs/modules/data_connection/document_transformers/character_text_splitter/) che permette di frazionare il testo tramite il modulo CharacterTextSplitter o RecursiveCharacterTextSplitter.

```js
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import MistralClient from "@mistralai/mistralai";
const client = new MistralClient(process.env.MISTRAL_API_KEY);

async function splitDocument(path) {
    const response = await fetch(path);
    const text = await response.text();
    const splitter = new RecursiveCharacterTextSplitter({
        chunkSize: 250,
        chunkOverlap: 40
    });
    const output = await splitter.createDocuments([text]);
    const textArr = output.map(chunk => chunk.pageContent);
    return textArr;
}

const handbookChunks = await splitDocument('handbook.txt');

async function createEmbeddings(chunks) {
    const embeddings = await client.embeddings({
        model: 'mistral-embed',
        input: chunks
    });
    const data = chunks.map((chunk, i) => {
        return {
            content: chunk,
            embedding: embeddings.data[i].embedding
        }
    });
    return data;
}

console.log(await createEmbeddings(handbookChunks));
```
## Vector DB
I database vettoriali hanno la capacitÃ  di conservare e recuperare velocemente e con una buona scalatura, dati in formato vettoriale. Quando si fa una query si cerca il vettore piÃ¹ simile al vettore della query e non si cerca per uguaglianza (come per i DB tradizionali). Oggi i piÃ¹ comuni sono:
- Faiss
- Chroma
- Pinecone
- [Supabase](https://supabase.com/)(DB Postgres con una estensione PG vector per storage embeddings ed eseguire vector similarity search). 
Nella voce "Extensions" si deve abilitare la voce "vector".
Per collegare l'applicazione con subabase si deve cliccare su project settings e copiare il Project_URL e la chiave privata e inserirle nel file .env.

```js

// data.js
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import MistralClient from "@mistralai/mistralai";
import { createClient } from "@supabase/supabase-js";

const mistalClient = new MistralClient(process.env.MISTRAL_API_KEY);
const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_API_KEY);

async function splitDocument(path) {
    const response = await fetch(path);
    const text = await response.text();
    const splitter = new RecursiveCharacterTextSplitter({
        chunkSize: 250,
        chunkOverlap: 40
    });
    const output = await splitter.createDocuments([text]);
    const textArr = output.map(chunk => chunk.pageContent);
    return textArr;
}

const handbookChunks = await splitDocument('handbook.txt');

async function createEmbeddings(chunks) {
    const embeddings = await mistalClient.embeddings({
        model: 'mistral-embed',
        input: chunks
    });
    const data = chunks.map((chunk, i) => {
        return {
            content: chunk,
            embedding: embeddings.data[i].embedding
        }
    });
    return data;
}

const data = await createEmbeddings(handbookChunks);
await supabase.from('handbook_docs').insert(data);  // inserisce i dati nel DB
console.log("Upload complete");
```


```js
import MistralClient from "@mistralai/mistralai";
import { createClient } from "@supabase/supabase-js";

const mistralClient = new MistralClient(process.env.MISTRAL_API_KEY);
const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_API_KEY);

// 1. Getting the user input
const input = "December 25th is on a Sunday, do I get any extra time off to account for that?";

// 2. Creating an embedding of the input
const embedding = await createEmbedding(input);

// 3. Retrieving similar embeddings / text chunks (aka "context")
const context = await retrieveMatches(embedding);

// 4. Combining the input and the context in a prompt 
// and using the chat API to generate a response 
const response = await generateChatResponse(context, input);
console.log(response);

async function createEmbedding(input) {
  const embeddingResponse = await mistralClient.embeddings({
      model: 'mistral-embed',
      input: [input]
  });
  return embeddingResponse.data[0].embedding;
}

// si cerca di recuperare i match piÃ¹ simili al prompt dell'utente
async function retrieveMatches(embedding) {
    const { data } = await supabase.rpc('match_handbook_docs', {
        query_embedding: embedding,
        match_threshold: 0.78,
        match_count: 5
    });
    return data.map(chunk => chunk.content).join(" ");
}


async function generateChatResponse(context, query) {
    const response = await mistralClient.chat({
        model: 'mistral-large-latest',
        messages: [{
            role: 'user',
            content: `Handbook context: ${context} - Question: ${query}`
        }]
    });
    return response.choices[0].message.content;
}

```

## AI Agents with Function Calling
Il function calling in un LLM (Large Language Model) Ã¨ la capacitÃ  del modello di riconoscere quando una richiesta dell'utente corrisponde a una funzione specifica e di generare automaticamente l'input corretto per chiamarla. Questo permette all'LLM di interagire con API, database o strumenti esterni in modo strutturato.

ğŸ“Œ Come funziona il function calling?
L'utente fa una richiesta â†’ L'LLM riconosce che la richiesta puÃ² essere soddisfatta chiamando una funzione.
L'LLM genera l'input corretto â†’ Costruisce un oggetto JSON o un'altra struttura compatibile con la funzione.
La funzione viene eseguita â†’ Il sistema esegue la funzione con i parametri forniti.
L'LLM restituisce il risultato â†’ PuÃ² spiegare il risultato all'utente o utilizzarlo per generare una risposta.

Esempi pratici
1ï¸âƒ£ Ottenere il meteo
Richiesta dell'utente:
ğŸ‘‰ "Che tempo fa oggi a Milano?"

L'LLM riconosce che deve chiamare una funzione per ottenere il meteo:

```json
{
  "function": "get_weather",
  "parameters": {
    "city": "Milano",
    "date": "2025-02-05"
  }
}
```
La funzione get_weather esegue la chiamata a un'API meteo e restituisce:

```json
{
  "temperature": "12Â°C",
  "condition": "Parzialmente nuvoloso"
}
```
L'LLM risponde all'utente:
"Oggi a Milano ci sono 12Â°C e il cielo Ã¨ parzialmente nuvoloso."

2ï¸âƒ£ Convertire valuta
Richiesta dell'utente:
ğŸ‘‰ "Quanto sono 100 dollari in euro?"

L'LLM genera:

```json
{
  "function": "convert_currency",
  "parameters": {
    "amount": 100,
    "from_currency": "USD",
    "to_currency": "EUR"
  }
}
```
La funzione ottiene il tasso di cambio e restituisce:

```json
{
  "converted_amount": 92.5
}
```
L'LLM risponde:
"100 dollari sono circa 92,5 euro con il tasso di cambio attuale."

Prenotare un appuntamento
Richiesta dell'utente:
ğŸ‘‰ "Prenotami un appuntamento dal dentista per domani alle 15:00."

L'LLM costruisce la richiesta:

```json
{
  "function": "book_appointment",
  "parameters": {
    "service": "dentista",
    "date": "2025-02-06",
    "time": "15:00"
  }
}
```
Il sistema verifica la disponibilitÃ  e conferma:

```json
{
  "status": "confermato",
  "location": "Studio Dentistico Rossi, Via Roma 10"
}
```
Risposta dell'LLM:
"Appuntamento confermato con il dentista per domani alle 15:00 presso Studio Dentistico Rossi, Via Roma 10."

Vantaggi del function calling:
- âœ… Risposte piÃ¹ precise â†’ L'LLM non deve "indovinare", ma usa dati aggiornati.
- âœ… Automazione â†’ L'utente puÃ² interagire con servizi esterni senza scrivere codice.
- âœ… Maggiore affidabilitÃ  â†’ Riduce errori e fraintendimenti rispetto a risposte solo testuali.

Questo approccio Ã¨ utile in chatbot avanzati, assistenti vocali, automazione aziendale e molto altro!

Come un LLM riconosce che una richiesta puÃ² essere soddisfatta chiamando una funzione? Un LLM che supporta il Function Calling Ã¨ addestrato o configurato per riconoscere quando una richiesta dell'utente puÃ² essere gestita da una funzione invece di generare solo testo. Questo processo si basa su diversi elementi chiave:

1ï¸âƒ£ Il modello Ã¨ istruito a identificare pattern di richieste
Gli LLM moderni, come quelli di OpenAI e altre implementazioni (ad es. LangChain + Ollama), sono addestrati su dataset che includono esempi di chiamate a funzioni.
ğŸ‘‰ Questo significa che il modello ha imparato a mappare richieste specifiche a funzioni predefinite.

ğŸ“Œ Esempio
Se il modello ha accesso alla funzione:

```json
{
  "name": "get_weather",
  "description": "Ottiene il meteo per una cittÃ ",
  "parameters": {
    "city": { "type": "string", "description": "Nome della cittÃ " }
  }
}
```
Allora, se l'utente dice:
ğŸ’¬ "Che tempo fa a Milano?"
L'LLM riconosce che puÃ² usare get_weather invece di generare una risposta basata su testo.

2ï¸âƒ£ Il modello viene istruito con un "schema" di funzioni (JSON o altro)
I framework di Function Calling (es. OpenAI, LangChain) forniscono all'LLM un elenco di funzioni disponibili, con i loro parametri e descrizioni.
Quando il modello riceve una richiesta, analizza se puÃ² essere mappata su una funzione registrata.

ğŸ”¹ Esempio di schema JSON per una funzione

```json
{
  "functions": [
    {
      "name": "get_exchange_rate",
      "description": "Ottiene il tasso di cambio tra due valute",
      "parameters": {
        "from_currency": { "type": "string", "description": "Valuta di partenza" },
        "to_currency": { "type": "string", "description": "Valuta di destinazione" }
      }
    }
  ]
}
```
Se l'utente chiede:
ğŸ’¬ "Quanto vale 100 dollari in euro?"
Il modello capisce che puÃ² chiamare get_exchange_rate("USD", "EUR").

3ï¸âƒ£ Matching semantico tra richiesta e funzioni
Il modello non cerca solo corrispondenze esatte, ma utilizza il significato della frase per capire se puÃ² invocare una funzione.

ğŸ”¹ Esempio con sinonimi e parafrasi

"Qual Ã¨ il meteo oggi a Roma?"
"Dimmi che tempo fa nella capitale d'Italia"
"Fa freddo a Roma oggi?"
ğŸ‘‰ Anche se le frasi sono diverse, il modello capisce che tutte richiedono get_weather("Roma").

4ï¸âƒ£ Score di confidenza e fallback
Se il modello Ã¨ molto sicuro che la richiesta puÃ² essere soddisfatta con una funzione, restituirÃ  direttamente la chiamata alla funzione.
Se Ã¨ incerto, puÃ² generare una risposta testuale o chiedere conferma all'utente.

ğŸ“Œ Esempio
Se l'utente dice:
ğŸ’¬ "Vorrei sapere il meteo..."
ğŸ‘‰ Il modello potrebbe rispondere:
"Di quale cittÃ  vuoi conoscere il meteo?"
(perchÃ© non ha abbastanza informazioni per chiamare la funzione get_weather).

# Links:
- [Scrimba course](https://scrimba.com/intro-to-mistral-ai-c035)


